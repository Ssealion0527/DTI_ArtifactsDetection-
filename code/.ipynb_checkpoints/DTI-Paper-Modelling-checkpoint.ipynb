{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ty-BC-bUC1CD",
    "outputId": "d4fdb2c7-be72-4663-9068-15e641c40605"
   },
   "outputs": [],
   "source": [
    "# !pip install nilearn\n",
    "# Import packages\n",
    "import nilearn\n",
    "from nilearn import image\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt #import a module\n",
    "from matplotlib.ticker import PercentFormatter #PercentFormatter is a function instead of a module\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import os\n",
    "import os, random, shutil\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "import re\n",
    "\n",
    "from scipy import ndimage, misc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Basic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and File Arrangement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##*****************Do not need to run it once we get the val,train,test folder, just for the first time***************\n",
    "\n",
    "# \n",
    "def moveFile(fileDir,tarDir_test_val,tarDir_train):\n",
    "    pathDir = os.listdir(fileDir)    #取图片的原始路径\n",
    "    filenumber= len(pathDir)\n",
    "    rate=0.2    #自定义抽取图片的比例，比方说100张抽10张，那就是0.1\n",
    "    picknumber=int(filenumber*rate) #按照rate比例从文件夹中取一定数量图片\n",
    "    random.seed(30)\n",
    "    sample = random.sample(pathDir, picknumber)  #随机选取picknumber数量的样本图片 For test and val\n",
    "    good_list = [x for x in pathDir if x not in sample]  ## remaining for the train\n",
    "    print(len(sample))\n",
    "    print(len(good_list))\n",
    "    count = 0\n",
    "    count1=0\n",
    "    \n",
    "    for f in enumerate(sample, 1):  ## move bad to bad_test(bad_val)/good to good_test(good_val)\n",
    "        dest = os.path.join(tarDir_test_val, str(f[1]))\n",
    "        start_from = os.path.join(fileDir, str(f[1]))\n",
    "        shutil.copy(start_from, dest)\n",
    "    for f in enumerate(good_list, 1): ## move remaining to bad_train/good to good_train\n",
    "        dest = os.path.join(tarDir_train, str(f[1]))\n",
    "        start_from = os.path.join(fileDir, str(f[1]))\n",
    "        shutil.copy(start_from, dest)\n",
    "    \n",
    "    return\n",
    "def moveFile2(fileDir,tarDir_test_val):\n",
    "    pathDir = os.listdir(fileDir)    #original path\n",
    "    filenumber= len(pathDir)\n",
    "    rate=0.2    \n",
    "    picknumber=int(filenumber*rate) \n",
    "    random.seed(30)\n",
    "    sample = random.sample(pathDir, picknumber)  #randomly pick pictures for test and val\n",
    "    good_list = [x for x in pathDir if x not in sample]  ## remaining for the train\n",
    "    print(len(sample))\n",
    "    print(len(good_list))\n",
    "    \n",
    "    for f in enumerate(sample, 1):  ## move bad to bad_test(bad_val)/good to good_test(good_val)\n",
    "        dest = os.path.join(tarDir_test_val, str(f[1]))\n",
    "        start_from = os.path.join(fileDir, str(f[1]))\n",
    "        shutil.move(start_from, dest)\n",
    "        \n",
    "    print(len(os.listdir(fileDir)))\n",
    "    print(len(os.listdir(tarDir_test_val)))\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting train/test/val data as array, preprocessing\n",
    "\n",
    "def extract_x(file_path_good,file_path_bad):y_label = []\n",
    "    index = 0\n",
    "    index_y = 0\n",
    "    maching_check = []\n",
    "    whole_candidate_good = os.listdir(file_path_good) ## in fact  very wired since must follow sequence s7.s11.1.2, key word in 1 too confused\n",
    "    whole_candidate_bad = os.listdir(file_path_bad) ## in fact  very wired since must follow sequence s7.s11.1.2, key word in 1 too confused\n",
    "  \n",
    "    for f in dataList:  ## traver by sequence s7,s11,1,2\n",
    "        data = [] ## store the array by subject(file)\n",
    "        filename = textfileList[index:index+1]\n",
    "      # print(filename, img.shape)\n",
    "        sub = filename[0].split('_man')[0]\n",
    "        if sub == '1':\n",
    "          sub = '1_vol'\n",
    "        img = f.get_data() # NiFTI to Numpy data type\n",
    "        sub_belong_good = [string for string in whole_candidate_good if (sub in string)]\n",
    "        sub_belong_bad = [string for string in whole_candidate_bad if (sub in string)]\n",
    "        # print('total_file:',len(sub_belong))\n",
    "        whole_candidate_good = [x for x in whole_candidate_good if x not in sub_belong_good]\n",
    "        whole_candidate_bad = [x for x in whole_candidate_bad if x not in sub_belong_bad]\n",
    "        # print('string:',sub)\n",
    "        for i in sub_belong_good:\n",
    "            y_label.append(1)\n",
    "            a = re.split('_vol',i)\n",
    "            b = a[1].split('_slice')\n",
    "            vol = int(b[0])-1\n",
    "            sli = int(b[1].split('.jpeg')[0])-1\n",
    "            k = img[:,:,sli,vol]\n",
    "            data.append(k)\n",
    "        for i in sub_belong_bad:\n",
    "            y_label.append(0)\n",
    "            a = re.split('_vol',i)\n",
    "            b = a[1].split('_slice')\n",
    "            vol = int(b[0])-1\n",
    "            sli = int(b[1].split('.jpeg')[0])-1\n",
    "            k = img[:,:,sli,vol]\n",
    "            data.append(k)\n",
    "        temp = np.dstack(data)\n",
    "        v_min = temp.min(axis=(0, 1), keepdims=True)\n",
    "        v_max = temp.max(axis=(0, 1), keepdims=True)\n",
    "        temp = (temp - v_min)/(v_max - v_min)\n",
    "        print(temp.shape)\n",
    "\n",
    "        # Down sampling to a same size, e.g., 96x96\n",
    "        img_new = zoom(temp, (minLength/temp.shape[0], minHeight/temp.shape[1], 1))\n",
    "\n",
    "        # Combine data\n",
    "        if index == 0:\n",
    "            x = img_new\n",
    "        else:\n",
    "            x = np.concatenate([x, img_new], axis=2)\n",
    "        index = index + 1\n",
    "\n",
    "    # end loop\n",
    "    x = np.moveaxis(x, -1, 0) #move the third dim to the first dim for running cnn which needs the first dim is the sample#\n",
    "    x = np.reshape(x, (x.shape[0],minLength, minHeight, 1)) #convert 2D sample to 3D, i.e., add 1 to the 3rd dimension\n",
    "    print(x.shape)\n",
    "    y_label = np.array(y_label)\n",
    "    return x, y_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation(arr,degree):\n",
    "    new_arr = ndimage.rotate(arr, degree, reshape=False)\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(x_train,y_train,degrees,ratio,approach):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    output:\n",
    "    \"\"\"\n",
    "    len_class_0 = sum(y_train==0)\n",
    "    len_class_1 = sum(y_train==1)\n",
    "    print('class 0',len_class_0)\n",
    "    print('class 1',len_class_1)\n",
    "    \n",
    "    num_sample = int(len_class_1*ratio/(1-ratio))-len_class_0 ## how many images we need to increase\n",
    "    print('Increased class 0 number is :', num_sample)\n",
    "\n",
    "    if approach == 'approach1':\n",
    "        # print('x_train.shape', x_train.shape)\n",
    "        # print(x_train)\n",
    "\n",
    "        \n",
    "        # ratio = 0.4   # the ratio determines the probability of negative samples\n",
    "        \n",
    "\n",
    "        select_slices_sum = num_sample/10 ## total 10 rotations degreees, this shows how many slices we need to select\n",
    "\n",
    "        class_0_index = np.argwhere(y_train == 0)\n",
    "        class_0_index = np.squeeze(class_0_index) ## array of index of all bad slices within whole training set\n",
    "        # print(class_0_index)\n",
    "\n",
    "        degrees = [-5,-4,-3,-2,-1,1,2,3,4,5] ##0-9 index\n",
    "        print('select_slices_sum',select_slices_sum)\n",
    "\n",
    "        random_index = random.sample(range(0,len_class_0),int(select_slices_sum)) ## randomly generated 912 indices within 1768 bad slices \n",
    "        print('random_index',len(random_index))\n",
    "\n",
    "        for i in random_index: ## traverse every randomly selected image index\n",
    "            # y_train_sample_index = random.choice(class_0_index)\n",
    "            y_train_sample_index = class_0_index[i] ## get corresponding index of bad slice within the whole training set\n",
    "            sampled_img = x_train[y_train_sample_index,:,:,:]\n",
    "            print('sampled_img.shape', sampled_img.shape)\n",
    "            print('select index:',y_train_sample_index)\n",
    "\n",
    "            for j in range(10): ## every image increase 10 images\n",
    "              aug = degrees[j]\n",
    "              # aug = random.choice(degrees)\n",
    "              print(aug)\n",
    "              sampled_img_new = rotation(sampled_img,aug)\n",
    "        \n",
    "              print('sampled_img.shape', sampled_img.shape)\n",
    "              sampled_img_new = sampled_img_new[np.newaxis,]\n",
    "              print('sampled_img_new.shape', sampled_img_new.shape)\n",
    "              x_train = np.concatenate([x_train, sampled_img_new], axis=0)\n",
    "\n",
    "              y_train = np.concatenate([y_train, [0]], axis=0)\n",
    "\n",
    "    if approach == 'approach2':\n",
    "        count=0\n",
    "        \n",
    "        \n",
    "        whole_group_1_degrees = int(num_sample/len_class_0)  ## how many augs every image need to increase (whole bad slices)\n",
    "        seperate_group_2 = num_sample-whole_group_1_degrees*len_class_0 ## how many remaining slices need to be augmented\n",
    "\n",
    "        class_0_index = np.argwhere(y_train == 0)\n",
    "        class_0_index = np.squeeze(class_0_index) ## array of index of all bad slices within whole training set\n",
    "        # print(class_0_index)\n",
    "\n",
    "\n",
    "        degrees = [-5,-4,-3,-2,-1,1,2,3,4,5] ##0-9 index\n",
    "\n",
    "        random_index_whole_group_degree = random.sample(range(0,10),int(whole_group_1_degrees)) ## randomly generated ~x degrees within total 10 degree\n",
    "        select_degrees = []\n",
    "        print('first part',random_index_whole_group_degree)\n",
    "        for i in class_0_index:   ## traverse every bad image within the whole training set\n",
    "          sampled_img = x_train[i,:,:,:]\n",
    "          # print('sampled_img.shape', sampled_img.shape)\n",
    "          # print('select index:',i)\n",
    "\n",
    "          for j in random_index_whole_group_degree:\n",
    "            aug_degree = degrees[j]\n",
    "            select_degrees.append(aug_degree)\n",
    "            print(aug_degree)\n",
    "            sampled_img_new = rotation(sampled_img,aug_degree)\n",
    "\n",
    "            # print('sampled_img.shape', sampled_img.shape)\n",
    "            sampled_img_new = sampled_img_new[np.newaxis,]\n",
    "            # print('sampled_img_new.shape', sampled_img_new.shape)\n",
    "            count+=1\n",
    "            x_train = np.concatenate([x_train, sampled_img_new], axis=0)\n",
    "            y_train = np.concatenate([y_train, [0]], axis=0)\n",
    "\n",
    "        random_index = random.sample(range(0,len_class_0),int(seperate_group_2)) ## randomly generated remaining number indices within 1768 bad slices \n",
    "        remain_degrees = [item for item in degrees if item not in select_degrees]\n",
    "\n",
    "        print('random_index',random_index)\n",
    "        print('remain_degrees',remain_degrees)\n",
    "        for i in random_index: ## traverse every randomly selected image index\n",
    "            y_train_sample_index = class_0_index[i] ## get corresponding index of bad slice within the whole training set\n",
    "            sampled_img = x_train[y_train_sample_index,:,:,:]\n",
    "            # print('sampled_img.shape', sampled_img.shape)\n",
    "            # print('select index:',y_train_sample_index)\n",
    "\n",
    "\n",
    "            aug = random.choice(remain_degrees)\n",
    "            print(aug)\n",
    "            sampled_img_new = rotation(sampled_img,aug)\n",
    "            # print('sampled_img.shape', sampled_img.shape)\n",
    "            sampled_img_new = sampled_img_new[np.newaxis,]\n",
    "            # print('sampled_img_new.shape', sampled_img_new.shape)\n",
    "            count+=1\n",
    "            x_train = np.concatenate([x_train, sampled_img_new], axis=0)\n",
    "            y_train = np.concatenate([y_train, [0]], axis=0)\n",
    "\n",
    "    print('*********finished***********')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###***************          VERY IMPORTANT!!!! For switching the label, postive class for bad slices(label 1),negative class for good slices(label 0)\n",
    "## ***************     Reference here: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/\n",
    "\n",
    "def reverse_array(arrr):\n",
    "  for i in range(len(arrr)):\n",
    "    if arrr[i]==0:\n",
    "      arrr[i]=1\n",
    "    else:\n",
    "      arrr[i]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotmodel(trainResult,savedFigName='train_result.png'):  \n",
    "    train_acc = trainResult.history['accuracy'][::100]\n",
    "    val_acc = trainResult.history['val_accuracy'][::100]\n",
    "    train_loss = trainResult.history['loss'][::100]\n",
    "    val_loss = trainResult.history['val_loss'][::100]\n",
    "    \n",
    "    figsize_width=15 #inches\n",
    "    figsize_height=5 #inches\n",
    "    x_epochs_ticks_interval=round(len(train_acc)/figsize_width)\n",
    "    x_epochs = np.arange(1, len(train_acc) + 1, 1) #in the figure, eopch counts from 1\n",
    "    x_epochs_ticks=np.arange(1, len(train_acc) + 1,x_epochs_ticks_interval)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(figsize_width, figsize_height)) \n",
    "\n",
    "    #left one\n",
    "    ax[0].set_title('Accuracy')\n",
    "    ax[0].plot(x_epochs, train_acc, color='green', label='Train')\n",
    "    ax[0].plot(x_epochs, val_acc, color='red', label='Validation')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_xticks(x_epochs_ticks)\n",
    "    ax[0].set_xlabel('Epoch(per 100 epochs)')\n",
    "    ax[0].set_ylabel('Accuracy')\n",
    "    ax[0].yaxis.set_major_formatter(PercentFormatter(1))\n",
    "  \n",
    "    #right one\n",
    "    ax[1].set_title('Loss')\n",
    "    ax[1].plot(x_epochs, train_loss, color='green', label='Train')\n",
    "    ax[1].plot(x_epochs, val_loss, color='red', label='Validation')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xticks(x_epochs_ticks)\n",
    "    ax[1].set_xlabel('Epoch(per 100 epochs)')\n",
    "    ax[1].set_ylabel('Loss')  \n",
    "\n",
    "    fig.savefig(savedFigName)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "def plotcon(model, savedFigName='confusion_matrix.png'):\n",
    "    pred = model.predict_classes(x_test) # get predicted values. \n",
    "    con_mat = tf.math.confusion_matrix(labels=y_test, predictions=pred).numpy()\n",
    "    con_mat_df = pd.DataFrame(con_mat, index = [0, 1], columns = [0, 1]) # create a dataframe for plotting\n",
    "\n",
    "    # Show confusion matrix\n",
    "    figure = plt.figure(figsize=(6, 8))\n",
    "    sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues, fmt='g')\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.savefig(savedFigName)\n",
    "\n",
    "# ROC Curve 1\n",
    "def plotroc(model, savedFigName='roc_curve.png'):\n",
    "    fpr, tpr, threshold = roc_curve(y_test, model.predict(x_test))\n",
    "    auc_result = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, label='CNN (area = {:.3f})'.format(auc_result))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    plt.savefig(savedFigName)\n",
    "\n",
    "# ROC Curve 2\n",
    "def plotroc(model, savedFigName='roc_curve.png'):\n",
    "    yhat = model.predict_proba(x_test)\n",
    "    # retrieve just the probabilities for the positive class\n",
    "    pos_probs = yhat[:, 0]\n",
    "    # plot no skill roc curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    pyplot.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n",
    "    # calculate roc curve for model\n",
    "    fpr, tpr, _ = roc_curve(y_test, pos_probs)\n",
    "    auc_result = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label='CNN (area = {:.3f})'.format(auc_result))\n",
    "    # plot model roc curve  \n",
    "    # axis labels\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    \n",
    "\n",
    "def plotroc(model, savedFigName='PC_curve.png'):\n",
    "    yhat = model.predict(x_test)\n",
    "    model_probs = yhat[:, 0]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, model_probs)\n",
    "    auc_score = auc(recall, precision)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot([0, 1], [0, 0], 'k--')\n",
    "    plt.plot(recall, precision, label='Keras (area = {:.3f})'.format(auc_score))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    plt.savefig(savedFigName)\n",
    "\n",
    "# P-C Curve 2\n",
    "# plot no skill and model precision-recall curves\n",
    "def plot_pr_curve(test_y, model_probs):\n",
    "\t# calculate the no skill line as the proportion of the positive class\n",
    "\tno_skill = len(test_y[test_y==1]) / len(test_y)\n",
    "\t# plot the no skill precision-recall curve\n",
    "\tpyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "\t# plot model precision-recall curve\n",
    "\tprecision, recall, _ = precision_recall_curve(test_y, model_probs)\n",
    "\tpyplot.plot(recall, precision, marker='.', label='Precision-Recall')\n",
    "\t# axis labels\n",
    "\tpyplot.xlabel('Recall')\n",
    "\tpyplot.ylabel('Precision')\n",
    "\t# show the legend\n",
    "\tpyplot.legend()\n",
    "\t# show the plot\n",
    "\tpyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SrKOYcj0DCFV",
    "outputId": "d24515ba-2b31-4849-8a76-8643dfef0858"
   },
   "outputs": [],
   "source": [
    "rootpath=r'rootpath'\n",
    "subpath=r'subpath'\n",
    "save_path = r'save_path'\n",
    "save_path2 = r'save_path2'\n",
    "drive.mount(rootpath)\n",
    "\n",
    "save_path = os.path.join(rootpath,save_path)\n",
    "save_path2 = os.path.join(rootpath,save_path2)\n",
    "datapath= os.path.join(rootpath,subpath)\n",
    "os.chdir(save_path)\n",
    "os.chdir(save_path2)\n",
    "os.chdir(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sLAd9QBFf4S8",
    "outputId": "531800ed-1fc3-48ff-ad34-2e7d4fac4dd5"
   },
   "outputs": [],
   "source": [
    "# Get text files\n",
    "textfileList = []\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".txt\"):\n",
    "        textfileList.append(file)\n",
    "textfileList = textfileList[:7]\n",
    "\n",
    "# Get nii file names\n",
    "nameList = []\n",
    "for file in textfileList:\n",
    "    nameList.append(file.split('.')[0][:-4])\n",
    "\n",
    "# Import data\n",
    "dataList = []\n",
    "index = 0\n",
    "for file in nameList:\n",
    "    if os.path.exists(file + '.nii'):\n",
    "        img = image.smooth_img(os.path.join(datapath, file + '.nii'), fwhm=None)\n",
    "        dataList.append(img)\n",
    "    elif os.path.exists(file + '.nii.gz'):\n",
    "        img = image.smooth_img(os.path.join(datapath, file + '.nii.gz'), fwhm=None)\n",
    "        dataList.append(img)\n",
    "    else:\n",
    "        print(\"No corresponding data file\")\n",
    "        del textfileList[index]\n",
    "        continue\n",
    "    index = index + 1\n",
    "\n",
    "# Get shape and get minimum dimensions\n",
    "minLength = []\n",
    "minHeight = []\n",
    "slices = []\n",
    "for file in dataList:\n",
    "    print(file.shape)\n",
    "    minLength.append(file.shape[0])\n",
    "    minHeight.append(file.shape[1])\n",
    "    slices.append(file.shape[2]*file.shape[3])\n",
    "\n",
    "minLength = min(minLength)\n",
    "minHeight = min(minHeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLoB8t_Bf4VZ"
   },
   "outputs": [],
   "source": [
    "tar_path_train_bad = r'tar_path_train_bad'\n",
    "tar_path_train_good = r'tar_path_train_good'\n",
    "\n",
    "tar_path_test_bad = r'tar_path_test_bad'\n",
    "tar_path_test_good = r'tar_path_test_good'\n",
    "\n",
    "tar_path_val_bad = r'tar_path_val_bad'\n",
    "tar_path_val_good = r'tar_path_val_good'\n",
    "\n",
    "\n",
    "tar_path_train_bad = os.path.join(rootpath,tar_path_train_bad)\n",
    "tar_path_train_good = os.path.join(rootpath,tar_path_train_good)\n",
    "tar_path_test_bad = os.path.join(rootpath,tar_path_test_bad)\n",
    "tar_path_test_good = os.path.join(rootpath,tar_path_test_good)\n",
    "tar_path_val_bad = os.path.join(rootpath,tar_path_val_bad)\n",
    "tar_path_val_good = os.path.join(rootpath,tar_path_val_good)\n",
    "\n",
    "os.chdir(tar_path_train_bad)\n",
    "os.chdir(tar_path_train_good)\n",
    "os.chdir(tar_path_test_bad)\n",
    "os.chdir(tar_path_test_good)\n",
    "os.chdir(tar_path_train_bad)\n",
    "os.chdir(tar_path_val_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset and Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3y0MYZsf4Zj"
   },
   "outputs": [],
   "source": [
    "moveFile(save_path,tar_path_test_bad,tar_path_train_bad) ## train test split\n",
    "moveFile(save_path2,tar_path_test_good,tar_path_train_good) ## train test split\n",
    "moveFile2(tar_path_train_bad,tar_path_val_bad) ## test val split\n",
    "moveFile2(tar_path_train_good,tar_path_val_good) ## test val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xg6DY_DPfLev",
    "outputId": "45aa82ac-3c6a-45ea-de0a-a6bc636fcbf1"
   },
   "outputs": [],
   "source": [
    "x_val,y_val = extract_x(tar_path_val_good,tar_path_val_bad)\n",
    "\n",
    "x_train,y_train = extract_x(tar_path_train_good,tar_path_train_bad)\n",
    "x_test,y_test = extract_x(tar_path_test_good,tar_path_test_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "J7VHPhy9qcgR",
    "outputId": "6c2b8c2a-610d-4252-bb1c-8eb95e590877"
   },
   "outputs": [],
   "source": [
    "len_class_0 = sum(y_train==0)\n",
    "len_class_1 = sum(y_train==1)\n",
    "\n",
    "ratio = 0.4   # the ratio determines the probability of negative samples\n",
    "num_sample = int(len_class_1*ratio/(1-ratio))-len_class_0 ## how many images we need to increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auguention on imbalanced data for training\n",
    "degrees = [-5,-4,-3,-2,-1,1,2,3,4,5]\n",
    "ratio = 0.5\n",
    "approach = 'approach1'\n",
    "data_aug(x_train,y_train,degrees,ratio,approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "D42zqbzS2hgD",
    "outputId": "d2dc4014-f039-4329-bc62-22577aec71c7"
   },
   "outputs": [],
   "source": [
    "## check the current label ratio before sending to the model\n",
    "### bad\n",
    "print(\"Train Proportion:\", sum(y_train == 0) / len(y_train))\n",
    "print(\"Val Proportion:\", sum(y_val == 0) / len(y_val))\n",
    "print(\"Test Proportion:\", sum(y_test == 0) / len(y_test))\n",
    "\n",
    "## good\n",
    "print(\"Train Proportion:\", sum(y_train == 1) / len(y_train))\n",
    "print(\"Val Proportion:\", sum(y_val == 1) / len(y_val))\n",
    "print(\"Test Proportion:\", sum(y_test == 1) / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEhabPo7tXnj"
   },
   "outputs": [],
   "source": [
    "###***************          VERY IMPORTANT!!!! For switching the label, postive class for bad slices(label 1),negative class for good slices(label 0)\n",
    "## ***************     Reference here: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/\n",
    "reverse_array(y_train)\n",
    "reverse_array(y_val)\n",
    "reverse_array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "mKf8EH4cxT4H",
    "outputId": "5a9be6fc-bbd9-4fb9-9be7-cd42de0c26c1"
   },
   "outputs": [],
   "source": [
    "## check the current label ratio before sending to the model\n",
    "### bad\n",
    "print(\"Train Proportion:\", sum(y_train == 1) / len(y_train))\n",
    "print(\"Val Proportion:\", sum(y_val == 1) / len(y_val))\n",
    "print(\"Test Proportion:\", sum(y_test == 1) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kliBPFmpxT6v"
   },
   "outputs": [],
   "source": [
    "# Paper CNN Model\n",
    "def cnn_model():\n",
    "  model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv2D(17, (7,7),input_shape =(minLength, minHeight, 1), activation='relu', padding='valid'),\n",
    "      tf.keras.layers.MaxPooling2D(pool_size=(4, 4),strides=(4, 4), padding='valid'),\n",
    "      tf.keras.layers.Conv2D(7, (5,5), activation='relu',padding='valid'),\n",
    "      tf.keras.layers.MaxPooling2D(pool_size=(4, 4),strides=(4, 4), padding='valid'), \n",
    "      tf.keras.layers.Dropout(0.4),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(100, kernel_regularizer=regularizers.l2(0.08),activation='relu'),\n",
    "      # tf.keras.layers.Dropout(0.4),\n",
    "      tf.keras.layers.Dense(1,kernel_regularizer=regularizers.l2(0.08),activation='sigmoid')\n",
    "      #tf.keras.layers.Dense(2)\n",
    "  ])\n",
    "  # Model metrics\n",
    "  \n",
    "  adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "  model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "6cYMpVm6xT9U",
    "outputId": "e539fa86-86c5-47e3-bcf6-1aae6ac84d2a"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "N_mJfbwAiJwl",
    "outputId": "7fdc382c-4913-455f-900f-04b926a35b60"
   },
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GbWRsuGsxUDm",
    "outputId": "1f63560d-0345-43be-9a48-c5c3335a306f"
   },
   "outputs": [],
   "source": [
    "# Training model\n",
    "\n",
    "model = cnn_model()\n",
    "class_weight = {0:0.8333333333333334, 1:1.25}\n",
    "\n",
    "mycallbacks = [tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/content/drive/My Drive/model{epoch:08d}.h5', \n",
    "    period=100,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=1000)]\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=800, epochs=35000, callbacks=mycallbacks,validation_data=(x_val, y_val),class_weight=class_weight)\n",
    "\n",
    "results = model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "# save model and architecture to single file\n",
    "model.save(\"/content/drive/My Drive/final_model_methods1921.h5\")\n",
    "\n",
    "\n",
    "print(results)\n",
    "# Save training log into text file\n",
    "hist_df = pd.DataFrame(history.history) \n",
    "hist_df\n",
    "hist_df.to_csv(\"/content/drive/My Drive/train_result.txt\", sep='\\t', index=False)\n",
    "\n",
    "# Save testing log into text file\n",
    "hist_df = pd.DataFrame(results, index=['loss','accuracy']) \n",
    "hist_df\n",
    "hist_df.to_csv(\"/content/drive/My Drive/test_result.txt\", sep='\\t', index=False)\n",
    "\n",
    "\n",
    "# Plot the model accuracy\n",
    "plotmodel(history, os.path.join(datapath,'train_result.png'))\n",
    "# Plot the confusion matrix\n",
    "plotcon(model, os.path.join(datapath, 'confusion_matrix.png'))\n",
    "# Plot the roc curve 1\n",
    "plotroc(model, os.path.join(datapath, 'roc_curve.png'))\n",
    "# Plot the roc curve 2\n",
    "plotroc(model, os.path.join(datapath, 'roc_curve.png'))\n",
    "# Precision Recall Curve\n",
    "# calculate the precision-recall auc\n",
    "# P-C Curve 1\n",
    "plotroc(model, os.path.join(datapath, 'PC_curve.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GbWRsuGsxUDm",
    "outputId": "1f63560d-0345-43be-9a48-c5c3335a306f"
   },
   "outputs": [],
   "source": [
    "# no skill model, stratified random class predictions\n",
    "model1 = DummyClassifier(strategy='stratified')\n",
    "model1.fit(x_test, y_test)\n",
    "yhat = model1.predict_proba(x_test)\n",
    "naive_probs = yhat[:, 1]\n",
    "# calculate the precision-recall auc\n",
    "precision, recall, _ = precision_recall_curve(y_test, naive_probs)\n",
    "auc_score = auc(recall, precision)\n",
    "print('No Skill PR AUC: %.3f' % auc_score)\n",
    "# fit a model\n",
    "yhat = model.predict(x_test)\n",
    "model_probs = yhat[:, 0]\n",
    "# calculate the precision-recall auc\n",
    "precision, recall, _ = precision_recall_curve(y_test, model_probs)\n",
    "auc_score = auc(recall, precision)\n",
    "print('CNN AUC: %.3f' % auc_score)\n",
    "# plot precision-recall curves\n",
    "plot_pr_curve(y_test, model_probs)\n",
    "plt.savefig(os.path.join(datapath, 'PR_curve.png'))\n",
    "\n",
    "yhat_probs = model.predict(x_test, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = model.predict_classes(x_test, verbose=0)\n",
    "\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    "\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, yhat_classes)\n",
    "print('F1 score: %f' % f1)\n",
    "\n",
    "# kappa\n",
    "kappa = cohen_kappa_score(y_test, yhat_classes)\n",
    "print('Cohens kappa: %f' % kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noyIE1wSDCb7"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    " \n",
    "# load model\n",
    "model = load_model(\"/content/drive/My Drive/final_model.h5\")\n",
    "yhat_probs = model.predict_proba(x_test) ## show the example for the predict\n",
    "yhat = model.predict_proba(x_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "experiment_8_28.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
